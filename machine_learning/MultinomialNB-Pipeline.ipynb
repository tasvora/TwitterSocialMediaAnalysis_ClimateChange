{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_location = pd.read_csv('../data/graphAnalysis/clean_climateTwitterData.csv')\n",
    "tweet_df_location = tweet_df_location[tweet_df_location['text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step was performed in the sentiment Analyzer\n",
    "# def replace_urls(in_string, replacement=None):\n",
    "#     # \"\"\"Replace URLs in strings. See also: ``bit.ly/PyURLre``\n",
    "\n",
    "#     # Args:\n",
    "#     #     in_string (str): string to filter\n",
    "#     #     replacement (str or None): replacment text. defaults to '<-URL->'\n",
    "\n",
    "#     # Returns:\n",
    "#     #     str\n",
    "#     # \"\"\"\n",
    "#     replacement = '<-URL->' if replacement is None else replacement\n",
    "#     pattern = re.compile('(https?://)?(\\w*[.]\\w+)+([/?=&]+\\w+)*')\n",
    "#     return re.sub(pattern, replacement, in_string)\n",
    "\n",
    "\n",
    "def tokenize_only(in_string):\n",
    "    \"\"\"\n",
    "    Convert `in_string` of text to a list of tokens using NLTK's TweetTokenizer\n",
    "    \"\"\"\n",
    "    # reasonable, but adjustable tokenizer settings\n",
    "    tokenizer = TweetTokenizer(preserve_case=False,\n",
    "                               reduce_len=True,\n",
    "                               strip_handles=False)\n",
    "    tokens = tokenizer.tokenize(in_string)\n",
    "    return tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step was performed in the sentiment Analyzer\n",
    "# # remove urls and retweets and entities from the text\n",
    "# tweet_df_location['text_clean'] = tweet_df_location['text'].apply(lambda row:replace_urls(row))\n",
    "\n",
    "# #remove punctuations\n",
    "# RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])  \n",
    "# tweet_df_location['text_clean'] = tweet_df_location['text_clean'].str.replace(RE_PUNCTUATION, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'tis\", 'whom', ' she', 'how', ' if', \"won't\", \"she'll\", 'own', 'we', 'your', 'yours', 'in', \" must've\", ' ever', 'nor', ' but', \" when'd\", \"i'd\", 'should', ' so', ' only', 'those', 'after', 'isn', ' into', 'there', ' every', ' then', 'about', \" there's\", 'such', '#GreenNewDeal', 'don', ' either', ' were', \"you'll\", \" couldn't\", ' will', \" doesn't\", ' got', \" they've\", ' at', '#savetheplanet', ' must', 'between', \" when's\", \"why's\", \"didn't\", 'theirs', 'further', \" how'll\", 'yourself', 'up', 'doesn', ' them', ' who', \"they're\", ' let', 'http', \"shan't\", 'or', \" i'll\", ' twas', 'd', 'then', \"hasn't\", ' just', 'shan', 'ours', \"aren't\", \"where's\", 'wasn', ' in', 'for', \"i'm\", \"let's\", 'could', \" you'll\", 'am', ' their', 'all', 'other', \" why'd\", \"how's\", 'y', \"he's\", 'were', ' we', 'doing', 'off', 'and', \" i'm\", ' said', 'have', \" who'll\", 'couldn', 'been', \" you'd\", 'weren', ' some', \"doesn't\", 'but', 'each', 'both', ' hers', 'ma', 'our', \"when's\", \" shan't\", ' dear', ' any', ' had', \" you've\", ' its', 'haven', ' all', 'by', 'an', 'can', 's', 'its', 'to', ' should', \"couldn't\", \" he's\", \"he'll\", \"we'd\", \"that's\", 'now', '#globalwarming', ' did', '#climatestrike', \" i've\", ' from', ' among', 'me', ' was', 'of', 'having', ' this', \" mightn't\", 'that', 'because', 'my', \"wouldn't\", \"you've\", '#climateAction', '#bushfiresAustralia', \"weren't\", 'it', \" don't\", ' which', 'from', \"hadn't\", 're', 'their', \" who'd\", 'against', ' can', 'who', 'as', \" aren't\", \" how's\", 'what', ' a', 'did', ' whom', \" isn't\", 'before', '#ActOnClimate', 'the', ' is', ' our', 'needn', ' they', 'some', \"there's\", 'myself', \" they'll\", 'be', ' these', \" we're\", 'themselves', 'if', ' as', ' the', 'ought', ' may', \" you're\", 'where', ' on', '#climateStrike', 'are', \"wasn't\", 'while', ' an', 'shouldn', ' it', \" would've\", 'no', ' to', 'will', 'him', \" can't\", \" mustn't\", 'himself', ' when', ' else', 'not', ' says', '#climatecrisis', \"we're\", ' most', \" who's\", ' likely', 'during', ' or', \" why'll\", \"she'd\", ' how', '#GlobalWarming', ' after', 'once', \" wouldn't\", ' because', \"you're\", ' like', 'over', ' own', 'was', ' there', ' since', 'through', 'hers', 'same', 'any', 'them', \" she'd\", 'so', 'hadn', 'these', 'mustn', \"what's\", ' however', 'being', \" he'll\", ' wants', ' he', \"should've\", \"who's\", \"haven't\", 'm', 'yourselves', \" they'd\", \" we'd\", 'more', 'too', ' us', 'most', '#bushfires', \"they've\", 'she', 'into', \"we've\", ' be', ' nor', 'here', ' rather', 'won', \" might've\", ' could', ' neither', ' have', \" where'll\", 'his', \" why's\", 'again', \" weren't\", ' of', 'itself', ' her', ' would', \"here's\", \" i'd\", 'has', ' might', 'this', 'i', \"don't\", '#sustainability', 'when', \" hasn't\", ' why', 'does', 'with', 'll', \" they're\", \" he'd\", \" she's\", ' am', 'ain', \" where's\", 'is', 'they', 'below', \" wasn't\", 'until', \" we'll\", 'down', ' me', ' his', ' than', \" didn't\", \" that'll\", \" ain't\", \"he'd\", ' yet', ' tis', 'a', \"mustn't\", 'would', ' i', ' him', 'at', ' often', 'didn', 'few', \"we'll\", ' been', \"i'll\", ' cannot', 'under', 'you', \"i've\", \"isn't\", ' not', \" could've\", \" when'll\", \"needn't\", \" that's\", 'only', \" how'd\", \" what'd\", 'had', ' where', 'than', 'ourselves', 'o', \"you'd\", ' your', ' other', ' you', ' across', \" won't\", \"shouldn't\", \"they'll\", 'he', ' and', \"that'll\", \"it's\", \" should've\", ' do', 'aren', 'which', ' also', ' does', \" where'd\", 'why', 'https', 'just', \"she's\", 't', \" what's\", ' with', \" she'll\", ' what', ' say', 'very', 'do', \" it's\", ' no', ' least', '#environment', 've', \" 'twas\", ' off', ' has', 'wouldn', ' able', ' too', ' get', 'on', 'her', ' are', ' almost', 'hasn', '#climatechange', ' my', 'mightn', \" shouldn't\", 'above', ' by', 'herself', \"mightn't\", ' about', ' while', '#FridaysForFuture', 'out', ' for', \"they'd\", ' that']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# List of stopwords\n",
    "stop_words= stopwords.words('english') #import stopwords from NLTK package\n",
    "readInStopwords = pd.read_csv(\"pre_process/twitterStopWords.csv\", encoding='ISO-8859-1') # import stopwords from CSV file as pandas data frame\n",
    "readInStopwords = readInStopwords.wordList.tolist() # convert pandas data frame to a list\n",
    "readInStopwords.append('http')\n",
    "readInStopwords.append('https')\n",
    "\n",
    "search_terms = ['#climateStrike','#climatestrike','#climatechange','#GreenNewDeal','#climatecrisis','#climateAction','#FridaysForFuture',\n",
    "            '#environment','#globalwarming','#GlobalWarming','#ActOnClimate','#sustainability','#savetheplanet',\n",
    "        '#bushfiresAustralia','#bushfires']\n",
    "readInStopwords.extend(search_terms)\n",
    "stop_list = stop_words + readInStopwords # combine two lists i.e. NLTK stop words and CSV stopwords\n",
    "stop_list = list(set(stop_list)) # strore only unique values \n",
    "    \n",
    "print(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweet_df_location['text_clean']\n",
    "y = tweet_df_location['search_hashtags']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  5,  5, ..., 10, 10, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "encoded_y = label_encoder.transform(y)\n",
    "encoded_y.shape\n",
    "encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.00, stop_words=stop_list, tokenizer=tokenize_only) # Use tf (raw term count) features for LDA.\n",
    "#X_Vect = tf_vectorizer.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, encoded_y, test_size=0.30)#, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50683,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50683,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21722,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21722,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_pipe = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer(max_df=0.9, min_df=0.00, stop_words=stop_list, tokenizer=tokenize_only)),\n",
    "    ('clf',MNB(alpha=0.01))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tasne\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['#actonclimate', '#bushfiresaustralia', '#climateaction', '#fridaysforfuture', '#greennewdeal', \"'\", 'able', 'across', \"ain't\", 'almost', 'also', 'among', \"can't\", 'cannot', \"could've\", 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', \"how'd\", \"how'll\", 'however', 'least', 'let', 'like', 'likely', 'may', 'might', \"might've\", 'must', \"must've\", 'neither', 'often', 'rather', 'said', 'say', 'says', 'since', 'tis', 'twas', 'us', 'wants', \"what'd\", \"when'd\", \"when'll\", \"where'd\", \"where'll\", \"who'd\", \"who'll\", \"why'd\", \"why'll\", \"would've\", 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=0.9, max_features=None,\n",
       "                                 min_df=0.0, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=[\"'tis\", 'whom', ' she', 'how',\n",
       "                                             ' if', \"won't\", \"she'll\", 'own',\n",
       "                                             '...\n",
       "                                             \" must've\", ' ever', 'nor', ' but',\n",
       "                                             \" when'd\", \"i'd\", 'should', ' so',\n",
       "                                             ' only', 'those', 'after', 'isn',\n",
       "                                             ' into', 'there', ' every',\n",
       "                                             ' then', 'about', \" there's\", ...],\n",
       "                                 strip_accents=None, sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize_only at 0x000002113F092510>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_pipe.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_clf_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 282    0   17  122   19   59   10    3    3   24    0   10]\n",
      " [   0  142    2  106   19    6    1    1    1    0    0    0]\n",
      " [  22    4  712  502  105  448   36   53   23   27    5   35]\n",
      " [  37   51  302 3177  200  505  146   41  143  100   23   85]\n",
      " [  22   15   77  523  511  209   26   14   37   56    4   20]\n",
      " [  13    2  109  354   91 4705   24  183   22   41    6   12]\n",
      " [   3    5   48  349   24   60  684    6   26   11   22  135]\n",
      " [   1    2   37   76   24  454   23  251    6    5   10   12]\n",
      " [   0    3    8  235   26   53   57   14  845    4   12    5]\n",
      " [  11    0   30  257   51  120    5    2    7  883    2   12]\n",
      " [   1    0   13   84    5   37   65    6   18    2  157   19]\n",
      " [   5    0   46  308   10   37  113    5   28    9   16 1137]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.51      0.60       549\n",
      "           1       0.63      0.51      0.57       278\n",
      "           2       0.51      0.36      0.42      1972\n",
      "           3       0.52      0.66      0.58      4810\n",
      "           4       0.47      0.34      0.39      1514\n",
      "           5       0.70      0.85      0.77      5562\n",
      "           6       0.57      0.50      0.53      1373\n",
      "           7       0.43      0.28      0.34       901\n",
      "           8       0.73      0.67      0.70      1262\n",
      "           9       0.76      0.64      0.69      1380\n",
      "          10       0.61      0.39      0.47       407\n",
      "          11       0.77      0.66      0.71      1714\n",
      "\n",
      "    accuracy                           0.62     21722\n",
      "   macro avg       0.62      0.53      0.56     21722\n",
      "weighted avg       0.62      0.62      0.61     21722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.71      0.51      0.60       549\n",
    "           1       0.63      0.51      0.57       278\n",
    "           2       0.51      0.36      0.42      1972\n",
    "           3       0.52      0.66      0.58      4810\n",
    "           4       0.47      0.34      0.39      1514\n",
    "           5       0.70      0.85      0.77      5562\n",
    "           6       0.57      0.50      0.53      1373\n",
    "           7       0.43      0.28      0.34       901\n",
    "           8       0.73      0.67      0.70      1262\n",
    "           9       0.76      0.64      0.69      1380\n",
    "          10       0.61      0.39      0.47       407\n",
    "          11       0.77      0.66      0.71      1714\n",
    "\n",
    "    accuracy                           0.62     21722\n",
    "    macro avg      0.62      0.53      0.56     21722\n",
    "    weighted avg   0.62      0.62      0.61     21722\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6208452260381181"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy is -- 0.4743117576650401 without Tokenization and Stopwords\n",
    "- Accuracy is -- 0.4820458521314796 with Tokenization and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Train: 0.839\n",
      "MultinomialNB Test Acc: 0.621\n"
     ]
    }
   ],
   "source": [
    "print('MultinomialNB Train: %.3f' % text_clf_pipe.score(X_train, y_train))\n",
    "print('MultinomialNB Test Acc: %.3f' % text_clf_pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "strVal = [\"Cuyahoga county eliminates use of plastic bags\",\"Weather is still warm in winters and is not freezing\"]\n",
    "# Value = tf_vectorizer.transform(strVal)\n",
    "# Value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['#environment', '#globalwarming'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use this way to obtain the value from encoding\n",
    "inv_s = label_encoder.inverse_transform(text_clf_pipe.predict(strVal).astype(int).ravel())\n",
    "inv_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tasne\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tasne\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['#actonclimate', '#bushfiresaustralia', '#climateaction', '#fridaysforfuture', '#greennewdeal', \"'\", 'able', 'across', \"ain't\", 'almost', 'also', 'among', \"can't\", 'cannot', \"could've\", 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', \"how'd\", \"how'll\", 'however', 'least', 'let', 'like', 'likely', 'may', 'might', \"might've\", 'must', \"must've\", 'neither', 'often', 'rather', 'said', 'say', 'says', 'since', 'tis', 'twas', 'us', 'wants', \"what'd\", \"when'd\", \"when'll\", \"where'd\", \"where'll\", \"who'd\", \"who'll\", \"why'd\", \"why'll\", \"would've\", 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.66\n",
      "Best parameters:  {'clf__alpha': 0.01, 'tfidf__ngram_range': (1, 2), 'tfidf__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_svm = {'tfidf__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False),'clf__alpha': (1e-2, 1e-3)}\n",
    "gs_clf = GridSearchCV(text_clf_pipe, parameters_svm, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(gs_clf.best_score_))\n",
    "print(\"Best parameters: \", gs_clf.best_params_)\n",
    "# param_grid = {'alpha': [0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "# grid = GridSearchCV(MNB(), param_grid, cv=10)\n",
    "# grid.fit(X_train, y_train)\n",
    "# print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "# print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best cross-validation score: 0.67\n",
    "Best parameters:  {'clf__alpha': 0.01, 'tfidf__ngram_range': (1, 2), 'tfidf__use_idf': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 8])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.predict(strVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['#environment', '#globalwarming'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use this way to obtain the value from encoding\n",
    "inv_s = label_encoder.inverse_transform(gs_clf.predict(strVal).astype(int).ravel())\n",
    "inv_s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
