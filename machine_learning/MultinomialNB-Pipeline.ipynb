{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "import nltk\n",
    "from nltk import tokenize as tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_location = pd.read_csv('../data/graphAnalysis/clean_climateTwitterData.csv')\n",
    "tweet_df_location = tweet_df_location[tweet_df_location['text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step was performed in the sentiment Analyzer\n",
    "# def replace_urls(in_string, replacement=None):\n",
    "#     # \"\"\"Replace URLs in strings. See also: ``bit.ly/PyURLre``\n",
    "\n",
    "#     # Args:\n",
    "#     #     in_string (str): string to filter\n",
    "#     #     replacement (str or None): replacment text. defaults to '<-URL->'\n",
    "\n",
    "#     # Returns:\n",
    "#     #     str\n",
    "#     # \"\"\"\n",
    "#     replacement = '<-URL->' if replacement is None else replacement\n",
    "#     pattern = re.compile('(https?://)?(\\w*[.]\\w+)+([/?=&]+\\w+)*')\n",
    "#     return re.sub(pattern, replacement, in_string)\n",
    "\n",
    "\n",
    "def tokenize_only1(in_string):\n",
    "    \"\"\"\n",
    "    Convert `in_string` of text to a list of tokens using NLTK's TweetTokenizer\n",
    "    \"\"\"\n",
    "    # reasonable, but adjustable tokenizer settings\n",
    "    tokenizer = TweetTokenizer(preserve_case=False,\n",
    "                               reduce_len=True,\n",
    "                               strip_handles=False)\n",
    "    tokens = tokenizer.tokenize(in_string)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in tok.sent_tokenize(text) for word in tok.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step was performed in the sentiment Analyzer\n",
    "# # remove urls and retweets and entities from the text\n",
    "# tweet_df_location['text_clean'] = tweet_df_location['text'].apply(lambda row:replace_urls(row))\n",
    "\n",
    "# #remove punctuations\n",
    "# RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])  \n",
    "# tweet_df_location['text_clean'] = tweet_df_location['text_clean'].str.replace(RE_PUNCTUATION, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' where', ' have', '#savetheplanet', \" might've\", \" wouldn't\", ' own', 'yourselves', 'them', 'had', \"they've\", 'hers', ' for', \"doesn't\", 'him', \"we're\", 'no', \" couldn't\", 'up', \" don't\", 'because', 'some', 'm', \" we'd\", 'http', 'such', 'don', 'hasn', \"here's\", ' how', \" they're\", \"they're\", \" we'll\", 'ma', \"we'd\", \" what's\", 'of', ' since', 'do', 'doesn', ' about', \" why's\", \" it's\", ' wants', 'how', ' me', 'and', ' however', ' so', 'this', ' tis', ' if', '#bushfiresAustralia', 'has', \" we're\", 'if', \"she'll\", 've', 'in', ' but', ' other', 'out', \" she'd\", 'ourselves', ' that', \"he'll\", 'mightn', 'shouldn', ' its', 'its', 'to', ' been', 'after', ' only', ' because', ' what', 'an', 'all', '#FridaysForFuture', \" doesn't\", \"she's\", 'or', ' no', 't', 'below', 'any', 'you', ' and', 'just', 'the', \"you're\", ' able', ' cannot', ' with', \"needn't\", '#globalwarming', 'while', \"couldn't\", 'why', ' across', \" why'd\", ' say', 'he', ' am', ' to', ' this', '#ActOnClimate', \" how'd\", ' after', \"shouldn't\", \" you're\", 'isn', ' we', \"how's\", 's', 'himself', \" mightn't\", ' most', ' on', ' your', '#GlobalWarming', \" where's\", ' when', \"who's\", 'wouldn', 'doing', 'can', \" how's\", \"mustn't\", ' just', \" i've\", 'were', ' would', 'should', ' in', ' also', \" shan't\", 'could', 'itself', ' she', 'theirs', 'me', 'weren', \" she's\", 'd', ' than', 'having', ' might', \" didn't\", 'am', \"they'll\", ' as', \"you'll\", ' does', 'on', \"you'd\", \" you'll\", ' among', 'for', ' was', 'would', \" isn't\", \"you've\", 'does', 'o', 're', ' has', ' could', \"'tis\", \"he's\", 'against', \"haven't\", \" i'd\", 'did', 'as', 'myself', 'yours', 'haven', 'by', \"won't\", 'than', \" who'd\", 'over', '#climatecrisis', 'needn', ' a', 'been', \"when's\", ' like', ' yet', 'same', 'hadn', ' got', \" i'm\", ' these', ' dear', 'with', ' into', 'is', 'so', \"there's\", \"aren't\", 'aren', \" could've\", '#environment', 'couldn', '#climateStrike', ' off', \" hasn't\", 'being', '#GreenNewDeal', \" who's\", 'then', ' hers', ' him', ' the', 'into', ' nor', \"i've\", ' from', 'not', 'further', 'very', ' he', ' ever', 'won', \" wasn't\", ' why', ' while', \" can't\", ' is', ' some', 'have', \" 'twas\", ' be', 'most', \"weren't\", ' at', \" where'll\", 'didn', \" shouldn't\", 'will', \" ain't\", 'at', ' by', \" when's\", \"isn't\", ' are', 'y', \"don't\", ' they', 'those', 'only', '#climateAction', \"what's\", \" where'd\", ' not', ' an', \"i'd\", ' least', 'few', \" that'll\", 'about', 'down', \"that'll\", ' were', ' can', 'these', 'their', 'now', 'they', 'during', ' or', 'a', 'until', \" they've\", 'https', \"shan't\", '#climatestrike', \" he'd\", ' my', 'yourself', 'too', 'it', 'what', 'll', ' said', 'between', \" weren't\", \" what'd\", 'but', 'there', ' of', ' any', ' there', \" you've\", '#bushfires', 'off', \"hadn't\", ' let', \"they'd\", 'nor', ' must', ' rather', 'more', \" would've\", 'where', ' them', 'under', 'her', \" he's\", ' get', 'here', 'above', ' else', 'your', \"should've\", \" aren't\", 'wasn', \"where's\", ' her', \" who'll\", \"wouldn't\", \" how'll\", \" i'll\", 'before', ' us', \"it's\", ' then', ' too', ' did', ' almost', \"i'll\", 'other', 'themselves', 'we', ' every', \" you'd\", ' i', 'once', 'ain', \"that's\", \" when'd\", \"she'd\", '#climatechange', 'which', 'through', \"we'll\", \" won't\", \"we've\", \" she'll\", ' whom', \" he'll\", ' neither', \"wasn't\", 'ours', ' which', ' will', ' either', \"didn't\", ' you', ' should', \" why'll\", ' it', \"let's\", ' who', 'was', 'herself', \" that's\", ' his', 'our', \" should've\", ' may', \"i'm\", 'mustn', 'each', \"mightn't\", ' do', ' all', 'who', 'whom', \" mustn't\", ' often', ' their', 'shan', \" they'd\", 'his', ' our', \" there's\", 'are', 'she', 'i', ' likely', 'from', ' twas', \"he'd\", \"hasn't\", 'be', 'again', ' had', 'both', 'when', ' says', \" they'll\", \" when'll\", 'my', 'own', 'that', \"why's\", 'ought', '#sustainability', \" must've\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# List of stopwords\n",
    "stop_words= stopwords.words('english') #import stopwords from NLTK package\n",
    "readInStopwords = pd.read_csv(\"pre_process/twitterStopWords.csv\", encoding='ISO-8859-1') # import stopwords from CSV file as pandas data frame\n",
    "readInStopwords = readInStopwords.wordList.tolist() # convert pandas data frame to a list\n",
    "readInStopwords.append('http')\n",
    "readInStopwords.append('https')\n",
    "\n",
    "search_terms = ['#climateStrike','#climatestrike','#climatechange','#GreenNewDeal','#climatecrisis','#climateAction','#FridaysForFuture',\n",
    "            '#environment','#globalwarming','#GlobalWarming','#ActOnClimate','#sustainability','#savetheplanet',\n",
    "        '#bushfiresAustralia','#bushfires']\n",
    "readInStopwords.extend(search_terms)\n",
    "stop_list = stop_words + readInStopwords # combine two lists i.e. NLTK stop words and CSV stopwords\n",
    "stop_list = list(set(stop_list)) # strore only unique values \n",
    "    \n",
    "print(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweet_df_location['text_clean']\n",
    "y = tweet_df_location['search_hashtags']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  5,  5, ..., 10, 10, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "encoded_y = label_encoder.transform(y)\n",
    "encoded_y.shape\n",
    "encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.00, stop_words=stop_list, tokenizer=tokenize_only) # Use tf (raw term count) features for LDA.\n",
    "#X_Vect = tf_vectorizer.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, encoded_y, test_size=0.30)#, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50683,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50683,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21722,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21722,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_pipe = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer(max_df=0.9, min_df=0.00, stop_words=stop_list, tokenizer=tokenize_only)),\n",
    "    ('clf',MNB(alpha=0.01))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tasne\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'across', 'actonclimate', 'ai', 'almost', 'also', 'among', 'bushfires', 'bushfiresaustralia', 'ca', 'climateaction', 'climatechange', 'climatecrisis', 'climatestrike', 'dear', 'either', 'else', 'environment', 'ever', 'every', 'fridaysforfuture', 'get', 'globalwarming', 'got', 'greennewdeal', 'however', 'least', 'let', 'like', 'likely', 'may', 'might', 'must', \"n't\", 'need', 'neither', 'often', 'rather', 'said', 'savetheplanet', 'say', 'says', 'sha', 'since', 'sustainability', 'tis', 'twas', 'us', 'wants', 'wo', 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=0.9, max_features=None,\n",
       "                                 min_df=0.0, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=[' where', ' have',\n",
       "                                             '#savetheplanet', \" might've\",\n",
       "                                             \" wouldn't\", '...\n",
       "                                             'no', \" couldn't\", 'up', \" don't\",\n",
       "                                             'because', 'some', 'm', \" we'd\",\n",
       "                                             'http', 'such', 'don', 'hasn',\n",
       "                                             \"here's\", ' how', \" they're\", ...],\n",
       "                                 strip_accents=None, sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize_only at 0x0000028ED4A53598>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time text_clf_pipe.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_clf_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 273    0   21  105   29   74    7    5    6   34    2   10]\n",
      " [   0  112    1   99   16    9    3    1    1    0    0    0]\n",
      " [  26    1  711  446  102  467   30   41   16   28    9   63]\n",
      " [  34   46  292 3127  206  508  136   49  152  108   26  121]\n",
      " [  25   18   88  533  494  208   25   17   36   54    5   13]\n",
      " [   9    9  136  314   99 4700   14  169   19   49   12   12]\n",
      " [   3    3   50  368   27   62  720    4   26   17   27  138]\n",
      " [   2    3   42   55   15  504   16  242    6    5    7    7]\n",
      " [   1    4   13  174   30   57   32    7  867    7    6   13]\n",
      " [  13    0   25  213   49  122   10    9   12  937    5    8]\n",
      " [   1    0   12   81    1   32   61    6   23    1  182   30]\n",
      " [   0    0   61  269   14   30  132   12   28   13   22 1137]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.48      0.57       566\n",
      "           1       0.57      0.46      0.51       242\n",
      "           2       0.49      0.37      0.42      1940\n",
      "           3       0.54      0.65      0.59      4805\n",
      "           4       0.46      0.33      0.38      1516\n",
      "           5       0.69      0.85      0.76      5542\n",
      "           6       0.61      0.50      0.55      1445\n",
      "           7       0.43      0.27      0.33       904\n",
      "           8       0.73      0.72      0.72      1211\n",
      "           9       0.75      0.67      0.71      1403\n",
      "          10       0.60      0.42      0.50       430\n",
      "          11       0.73      0.66      0.70      1718\n",
      "\n",
      "    accuracy                           0.62     21722\n",
      "   macro avg       0.61      0.53      0.56     21722\n",
      "weighted avg       0.61      0.62      0.61     21722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.71      0.51      0.60       549\n",
    "           1       0.63      0.51      0.57       278\n",
    "           2       0.51      0.36      0.42      1972\n",
    "           3       0.52      0.66      0.58      4810\n",
    "           4       0.47      0.34      0.39      1514\n",
    "           5       0.70      0.85      0.77      5562\n",
    "           6       0.57      0.50      0.53      1373\n",
    "           7       0.43      0.28      0.34       901\n",
    "           8       0.73      0.67      0.70      1262\n",
    "           9       0.76      0.64      0.69      1380\n",
    "          10       0.61      0.39      0.47       407\n",
    "          11       0.77      0.66      0.71      1714\n",
    "\n",
    "    accuracy                           0.62     21722\n",
    "    macro avg      0.62      0.53      0.56     21722\n",
    "    weighted avg   0.62      0.62      0.61     21722\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6215818064634933"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy is -- 0.4743117576650401 without Tokenization and Stopwords\n",
    "- Accuracy is -- 0.4820458521314796 with Tokenization and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Train: 0.845\n",
      "MultinomialNB Test Acc: 0.622\n"
     ]
    }
   ],
   "source": [
    "print('MultinomialNB Train: %.3f' % text_clf_pipe.score(X_train, y_train))\n",
    "print('MultinomialNB Test Acc: %.3f' % text_clf_pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "strVal = [\"Cuyahoga county eliminates use of plastic bags\",\"Weather is still warm in winters and is not freezing\"]\n",
    "# Value = tf_vectorizer.transform(strVal)\n",
    "# Value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['#environment', '#globalwarming'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use this way to obtain the value from encoding\n",
    "inv_s = label_encoder.inverse_transform(text_clf_pipe.predict(strVal).astype(int).ravel())\n",
    "inv_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tasne\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tasne\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", 'able', 'across', 'actonclimate', 'ai', 'almost', 'also', 'among', 'bushfires', 'bushfiresaustralia', 'ca', 'climateaction', 'climatechange', 'climatecrisis', 'climatestrike', 'dear', 'either', 'else', 'environment', 'ever', 'every', 'fridaysforfuture', 'get', 'globalwarming', 'got', 'greennewdeal', 'however', 'least', 'let', 'like', 'likely', 'may', 'might', 'must', \"n't\", 'need', 'neither', 'often', 'rather', 'said', 'savetheplanet', 'say', 'says', 'sha', 'since', 'sustainability', 'tis', 'twas', 'us', 'wants', 'wo', 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.67\n",
      "Best parameters:  {'clf__alpha': 0.01, 'tfidf__ngram_range': (1, 2), 'tfidf__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_svm = {'tfidf__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False),'clf__alpha': (1e-2, 1e-3)}\n",
    "gs_clf = GridSearchCV(text_clf_pipe, parameters_svm, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(gs_clf.best_score_))\n",
    "print(\"Best parameters: \", gs_clf.best_params_)\n",
    "# param_grid = {'alpha': [0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "# grid = GridSearchCV(MNB(), param_grid, cv=10)\n",
    "# grid.fit(X_train, y_train)\n",
    "# print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "# print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best cross-validation score: 0.67\n",
    "Best parameters:  {'clf__alpha': 0.01, 'tfidf__ngram_range': (1, 2), 'tfidf__use_idf': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.predict(strVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['#environment', '#climatechange'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use this way to obtain the value from encoding\n",
    "inv_s = label_encoder.inverse_transform(gs_clf.predict(strVal).astype(int).ravel())\n",
    "inv_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5764202191326766"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect), ('tfidf', TfidfTransformer()), \n",
    "                             ('mnb', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(X_train, y_train)\n",
    "\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(X_test)\n",
    "\n",
    "np.mean(predicted_mnb_stemmed == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.16      0.28       566\n",
      "           1       0.78      0.20      0.32       242\n",
      "           2       0.63      0.21      0.31      1940\n",
      "           3       0.43      0.79      0.55      4805\n",
      "           4       0.70      0.15      0.24      1516\n",
      "           5       0.62      0.89      0.73      5542\n",
      "           6       0.70      0.39      0.50      1445\n",
      "           7       0.65      0.17      0.27       904\n",
      "           8       0.81      0.44      0.57      1211\n",
      "           9       0.87      0.46      0.61      1403\n",
      "          10       0.88      0.27      0.41       430\n",
      "          11       0.83      0.60      0.70      1718\n",
      "\n",
      "    accuracy                           0.58     21722\n",
      "   macro avg       0.73      0.39      0.46     21722\n",
      "weighted avg       0.65      0.58      0.54     21722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predicted_mnb_stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5764202191326766"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test,predicted_mnb_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['#environment', '#globalwarming'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_s = label_encoder.inverse_transform(text_mnb_stemmed.predict(strVal).astype(int).ravel())\n",
    "inv_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tasne\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.69\n",
      "Best parameters:  {'mnb__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False),'mnb__alpha': (1e-2, 1e-3)}\n",
    "gs_clf1 = GridSearchCV(text_mnb_stemmed, parameters_svm, n_jobs=-1)\n",
    "gs_clf1 = gs_clf1.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(gs_clf1.best_score_))\n",
    "print(\"Best parameters: \", gs_clf1.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
