{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function for Tokenization\n",
    "\n",
    "def tokenize_only(in_string):\n",
    "    \"\"\"\n",
    "    Convert `in_string` of text to a list of tokens using NLTK's TweetTokenizer\n",
    "    \"\"\"\n",
    "    # reasonable, but adjustable tokenizer settings\n",
    "    tokenizer = TweetTokenizer(preserve_case=False,\n",
    "                               reduce_len=True,\n",
    "                               strip_handles=False)\n",
    "    tokens = tokenizer.tokenize(in_string)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' either', 'just', ' can', \"that'll\", 'o', \" it's\", 'these', \"aren't\", ' have', 'when', \" he's\", ' him', \" he'll\", ' in', ' will', 'no', ' yet', 'more', ' might', '#GlobalWarming', 'as', 'through', \" doesn't\", 'once', ' cannot', '#ActOnClimate', 'by', ' than', 'ought', 'then', \"i'll\", \" aren't\", 'aren', 'would', 'hasn', 'about', ' or', \" wasn't\", ' at', ' all', ' however', ' across', ' tis', \" would've\", 'which', 'is', 'haven', 'won', \"hadn't\", ' other', 'weren', ' as', 'this', ' often', 'while', 'm', \"we're\", \" shouldn't\", \"won't\", ' would', 'out', ' may', 'yourself', 'all', 'if', ' should', 'but', '#sustainability', \" she'll\", 'had', \"mustn't\", 'only', \"he'll\", 'mightn', \" we'd\", \" how's\", \" what'd\", \"isn't\", \" that's\", 'doesn', 'he', ' did', \"we've\", 'theirs', ' like', \"you've\", 'http', ' whom', \"wasn't\", \" why'd\", '#GreenNewDeal', 'above', \" i'm\", \"i'm\", 'll', \" wouldn't\", \"they're\", 'its', 'few', 'same', ' to', 'needn', \" should've\", 'was', ' too', 'our', 'i', 'before', ' were', ' they', 'me', \"you're\", \" mightn't\", \"when's\", \"who's\", 'here', 'should', ' why', 'who', ' you', 'itself', 'and', \" you've\", \" they're\", 'further', ' this', \" mustn't\", ' where', ' them', \"they'd\", ' who', ' our', ' dear', ' is', ' we', 'from', 'am', 'than', \"where's\", \" 'twas\", 'have', 'they', 'both', ' let', 'down', 'why', ' on', 'own', 'myself', ' then', 'most', \" she's\", 'to', \" won't\", \"there's\", \"she'd\", \" must've\", ' say', 'my', \" could've\", 'ma', ' could', 'yours', ' since', 'https', ' with', \" i'll\", 'does', \" might've\", \"hasn't\", ' was', '#globalwarming', \" when's\", 'into', ' off', ' these', 're', 'are', \" where's\", 't', ' twas', 'do', \" can't\", 'such', ' of', 'wouldn', 'a', ' said', 'couldn', 'nor', ' her', 'up', 'the', \"needn't\", \" they'd\", ' has', ' by', 'were', '#bushfires', \"here's\", \" weren't\", \" what's\", 'so', \" isn't\", 'under', \"they've\", 'each', \" when'll\", 'of', ' hers', \"'tis\", 'doing', \" don't\", 'ours', ' which', ' able', \"shouldn't\", 'your', \"we'll\", ' also', ' just', ' its', ' any', \" you're\", 'not', 'shan', 'been', 'with', ' every', \" you'll\", ' says', ' it', ' and', ' from', \" who'd\", 'after', \"what's\", \" who's\", \"you'd\", 'over', ' how', 'be', ' me', ' he', 'has', 'having', 'how', 'against', ' be', '#environment', '#climatestrike', '#bushfiresAustralia', 'that', \"why's\", 'can', \" couldn't\", ' i', \" he'd\", \"should've\", \" where'll\", \"they'll\", ' most', 's', ' almost', 'don', ' does', ' had', 'she', \" didn't\", 'their', \"that's\", ' an', 'now', \" why'll\", 'those', 'shouldn', \"he's\", \" hasn't\", 'herself', ' nor', \" how'll\", ' your', 'hers', ' own', 'ain', 'did', ' his', 'ourselves', 'y', 'again', \" they've\", ' that', \" how'd\", 'his', 'it', 'any', ' ever', 'on', \"it's\", \"couldn't\", ' likely', ' when', ' the', ' us', \"we'd\", \" when'd\", \"didn't\", \"mightn't\", \"weren't\", ' my', 'there', ' am', 'them', \" i'd\", 'him', 'what', \"he'd\", ' been', \" shan't\", \"how's\", ' not', 'being', '#climateAction', 'between', 'd', ' what', ' because', 'hadn', ' wants', 'we', \"let's\", ' if', ' their', \"wouldn't\", \" you'd\", ' so', ' about', 'off', ' else', '#climateStrike', \" they'll\", \" where'd\", \" there's\", \" ain't\", \"she'll\", \"don't\", 'during', \" why's\", ' a', ' while', 'will', \"i've\", ' she', ' only', \"shan't\", 'very', \"she's\", '#savetheplanet', 'himself', ' for', \" who'll\", ' but', \" that'll\", ' got', ' least', 'in', '#climatecrisis', 'or', 'because', 'at', 've', \"doesn't\", 'you', 'where', ' rather', 'for', \"haven't\", 'didn', 'some', 'yourselves', '#climatechange', ' among', ' no', ' must', 'her', '#FridaysForFuture', \"you'll\", ' after', \"i'd\", 'below', \" i've\", ' there', ' get', \" we'll\", 'could', 'wasn', 'mustn', ' neither', 'an', 'too', 'isn', ' into', 'until', \" we're\", \" she'd\", ' some', 'other', 'themselves', 'whom', ' do', ' are']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of stopwords\n",
    "stop_words= stopwords.words('english') #import stopwords from NLTK package\n",
    "readInStopwords = pd.read_csv(\"pre_process/twitterStopWords.csv\", encoding='ISO-8859-1') # import stopwords from CSV file as pandas data frame\n",
    "readInStopwords = readInStopwords.wordList.tolist() # convert pandas data frame to a list\n",
    "\n",
    "readInStopwords.append('http')\n",
    "readInStopwords.append('https')\n",
    "\n",
    "search_terms = ['#climateStrike','#climatestrike','#climatechange','#GreenNewDeal','#climatecrisis','#climateAction','#FridaysForFuture',\n",
    "            '#environment','#globalwarming','#GlobalWarming','#ActOnClimate','#sustainability','#savetheplanet',\n",
    "        '#bushfiresAustralia','#bushfires']\n",
    "\n",
    "readInStopwords.extend(search_terms)\n",
    "stop_list = stop_words + readInStopwords # combine two lists i.e. NLTK stop words and CSV stopwords\n",
    "stop_list = list(set(stop_list)) # strore only unique values \n",
    "    \n",
    "print(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('twitterLinearSVCModel.pkl', 'rb') as f:\n",
    "    model = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#climatestrike'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([\"Its freezing out today may be need to drive my SUV more\"])[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
