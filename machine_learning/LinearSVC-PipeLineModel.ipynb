{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tasne\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_location = pd.read_csv('../data/graphAnalysis/clean_climateTwitterData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'author_id', 'text', 'retweets', 'permalink',\n",
       "       'date', 'formatted_date', 'favorites', 'mentions', 'hashtags', 'geo',\n",
       "       'urls', 'search_hashtags', 'location', 'text_clean',\n",
       "       'tb_sentiment_polarity', 'tb_sentiment_subjectivity',\n",
       "       'textBlob_sentiment', 'vader_compound', 'vader_pos', 'vader_neg',\n",
       "       'vader_neu', 'V_Sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_location.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function for Tokenization\n",
    "\n",
    "def tokenize_only(in_string):\n",
    "    \"\"\"\n",
    "    Convert `in_string` of text to a list of tokens using NLTK's TweetTokenizer\n",
    "    \"\"\"\n",
    "    # reasonable, but adjustable tokenizer settings\n",
    "    tokenizer = TweetTokenizer(preserve_case=False,\n",
    "                               reduce_len=True,\n",
    "                               strip_handles=False)\n",
    "    tokens = tokenizer.tokenize(in_string)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' off', ' there', ' an', 'there', 'these', 'at', ' get', 'wouldn', ' how', 'over', ' my', ' hers', ' were', 'any', 'o', 'ours', ' who', 'them', \" that's\", '#climateAction', 'his', \" how's\", '#climatechange', 'i', \"they'd\", ' might', \"who's\", '#bushfires', 'yours', 'about', ' may', \" how'd\", \"mightn't\", 'ought', 'it', 'so', 'only', \"i'd\", 'but', ' yet', 'such', '#GlobalWarming', ' is', 'mustn', \"haven't\", 'a', 'mightn', ' got', \"we'll\", \"what's\", 'won', 'further', ' we', \" they'd\", ' he', ' him', ' should', 'needn', 'be', ' for', ' by', ' to', 'and', \" should've\", 'not', \" who's\", '#GreenNewDeal', 'both', 'that', \"'tis\", ' had', 'down', \" weren't\", \"i'm\", 'am', \" must've\", 'hadn', \"he's\", \"i'll\", ' this', 'own', \"where's\", ' own', \"they've\", 'more', 'same', ' not', ' why', 'an', 'before', \"here's\", \" we'll\", \" might've\", ' she', \"you'll\", ' like', ' most', \"didn't\", \"we'd\", 't', 're', 'if', 'or', ' them', \" where'll\", 'himself', \" can't\", ' while', ' whom', \"wasn't\", \"isn't\", 'they', ' a', \" they've\", \" there's\", 'would', 'ourselves', \"he'll\", ' neither', ' the', ' least', 'ain', \" he'll\", 'shan', \" ain't\", \" she'll\", ' me', ' else', 'her', \" doesn't\", ' say', \"couldn't\", 'why', ' your', 'shouldn', 'theirs', 'the', \" how'll\", \"she'll\", ' or', ' any', 'herself', ' because', 'ma', 'whom', 'as', 'than', 'yourselves', 'has', ' only', ' you', \" when'll\", ' their', ' just', \" couldn't\", ' can', \" hasn't\", 'here', \"shan't\", '#climatecrisis', ' wants', ' likely', \"that's\", ' would', ' does', 'haven', \"mustn't\", ' its', \" who'll\", ' been', 'above', ' have', 'didn', \" shan't\", 'been', \" wasn't\", \" could've\", ' cannot', 'while', 'were', \"needn't\", 'our', \" i've\", ' from', \" why'd\", ' almost', ' with', ' too', '#savetheplanet', 'between', \" you'd\", 'was', ' they', 'their', \" we'd\", 'through', 'where', \" why's\", 'your', \" i'm\", \" they're\", \"aren't\", \"let's\", '#globalwarming', \" would've\", 'did', 'we', ' of', \" mustn't\", \"you've\", 'each', 'do', 'he', 'from', ' do', ' said', \"that'll\", ' and', ' where', ' every', \" wouldn't\", 've', 'some', ' among', 'm', 'all', \"should've\", 'y', \" when's\", '#bushfiresAustralia', ' about', \" i'll\", ' than', ' be', ' some', ' also', ' i', \" 'twas\", ' on', 'now', \"there's\", 'nor', 'by', \" she'd\", 'off', \" he's\", ' no', 'couldn', \"you'd\", \"shouldn't\", ' twas', \"she's\", \"we've\", ' as', ' could', \"they'll\", ' tis', ' able', 'him', 'very', \" where's\", 'is', \"won't\", 'yourself', ' then', \" i'd\", ' into', 'what', ' will', \"i've\", \" shouldn't\", ' these', 'its', 'for', \"he'd\", 'during', \"why's\", \" what's\", ' was', '#sustainability', ' all', 'll', 'how', 'had', \" what'd\", '#environment', \" didn't\", \"she'd\", 'on', \" don't\", ' after', ' his', ' are', 'in', \"we're\", 'having', \" it's\", ' when', 'against', \" he'd\", 'aren', \"wouldn't\", \" aren't\", 'd', ' if', \"don't\", 'when', ' across', ' other', \"hasn't\", \" who'd\", 'because', \"when's\", 'wasn', ' did', 'does', 'no', 'with', 'you', \"weren't\", ' which', 'my', 'those', ' ever', 'http', \" she's\", 'into', 'few', 'don', \" we're\", 'should', 'below', 'out', ' however', \"doesn't\", \" won't\", \"you're\", 'most', 'she', \"how's\", 's', ' our', ' at', \"they're\", 'again', \" you've\", ' us', 'hers', 'too', 'have', ' often', '#FridaysForFuture', 'of', \" where'd\", \"hadn't\", \" isn't\", ' so', 'can', 'isn', ' am', '#climatestrike', ' what', ' rather', 'under', ' her', \"it's\", 'then', 'itself', ' nor', 'hasn', 'themselves', 'just', ' dear', 'myself', ' says', 'this', '#ActOnClimate', 'until', 'will', \" you're\", 'weren', ' since', ' has', ' but', '#climateStrike', ' that', 'me', 'could', ' in', 'after', 'to', ' must', 'https', \" that'll\", ' let', 'which', 'up', 'once', \" you'll\", \" they'll\", \" why'll\", ' either', 'doing', 'doesn', ' it', \" when'd\", 'being', \" mightn't\", 'are', 'who', 'other']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# List of stopwords\n",
    "stop_words= stopwords.words('english') #import stopwords from NLTK package\n",
    "readInStopwords = pd.read_csv(\"pre_process/twitterStopWords.csv\", encoding='ISO-8859-1') # import stopwords from CSV file as pandas data frame\n",
    "readInStopwords = readInStopwords.wordList.tolist() # convert pandas data frame to a list\n",
    "\n",
    "readInStopwords.append('http')\n",
    "readInStopwords.append('https')\n",
    "\n",
    "search_terms = ['#climateStrike','#climatestrike','#climatechange','#GreenNewDeal','#climatecrisis','#climateAction','#FridaysForFuture',\n",
    "            '#environment','#globalwarming','#GlobalWarming','#ActOnClimate','#sustainability','#savetheplanet',\n",
    "        '#bushfiresAustralia','#bushfires']\n",
    "\n",
    "readInStopwords.extend(search_terms)\n",
    "stop_list = stop_words + readInStopwords # combine two lists i.e. NLTK stop words and CSV stopwords\n",
    "stop_list = list(set(stop_list)) # strore only unique values \n",
    "    \n",
    "print(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweet_df_location['text_clean']\n",
    "y = tweet_df_location['search_hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.00, stop_words=stop_list, tokenizer=tokenize_only)\n",
    "\n",
    "#X_Vect = tf_vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer(max_df=0.9, min_df=0.00, stop_words=stop_list, tokenizer=tokenize_only)),\n",
    "    ('clf',LinearSVC(C=0.01))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tasne\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['#actonclimate', '#bushfiresaustralia', '#climateaction', '#fridaysforfuture', '#greennewdeal', \"'\", 'able', 'across', \"ain't\", 'almost', 'also', 'among', \"can't\", 'cannot', \"could've\", 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', \"how'd\", \"how'll\", 'however', 'least', 'let', 'like', 'likely', 'may', 'might', \"might've\", 'must', \"must've\", 'neither', 'often', 'rather', 'said', 'say', 'says', 'since', 'tis', 'twas', 'us', 'wants', \"what'd\", \"when'd\", \"when'll\", \"where'd\", \"where'll\", \"who'd\", \"who'll\", \"why'd\", \"why'll\", \"would've\", 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=0.9, max_features=None,\n",
       "                                 min_df=0.0, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=[' off', ' there', ' an', 'there',\n",
       "                                             'these', 'at', ' get', 'wouldn',...\n",
       "                                 strip_accents=None, sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize_only at 0x0000028154C01400>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LinearSVC(C=0.01, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 399    1   11   55   28   28   13    1    1   21    0   10]\n",
      " [   0  207    0   55    3    0    2    0    3    0    0    0]\n",
      " [   8    0 1218  253   57  250   30    3   26   34    4   30]\n",
      " [   9    2   32 4356   18  217   28    2   94   39    3   57]\n",
      " [   6    8   24  193 1046  109   18    0   28   51    2   10]\n",
      " [   5    4   10   91   10 5294   23    7   19   23    4   17]\n",
      " [   0    0    5   96    3    8 1202    0   20    3    1   73]\n",
      " [   5    4   33   56    9  353   15  418    5    6    4    3]\n",
      " [   0    1    1  105    0   17   24    1 1079    5    0   13]\n",
      " [   0    0    3   56    0   44    1    1    5 1259    1    7]\n",
      " [   0    0    3   20    3   12   25    0   16    2  334   15]\n",
      " [   0    0    0   21    2    5   11    0   16    1    1 1680]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    #actonclimate       0.92      0.70      0.80       568\n",
      "       #bushfires       0.91      0.77      0.83       270\n",
      "   #climateaction       0.91      0.64      0.75      1913\n",
      "   #climatechange       0.81      0.90      0.85      4857\n",
      "   #climatecrisis       0.89      0.70      0.78      1495\n",
      "   #climatestrike       0.84      0.96      0.89      5507\n",
      "     #environment       0.86      0.85      0.86      1411\n",
      "#fridaysforfuture       0.97      0.46      0.62       911\n",
      "   #globalwarming       0.82      0.87      0.84      1246\n",
      "    #greennewdeal       0.87      0.91      0.89      1377\n",
      "   #savetheplanet       0.94      0.78      0.85       430\n",
      "  #sustainability       0.88      0.97      0.92      1737\n",
      "\n",
      "         accuracy                           0.85     21722\n",
      "        macro avg       0.89      0.79      0.82     21722\n",
      "     weighted avg       0.86      0.85      0.85     21722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                       precision    recall  f1-score   support\n",
    "\n",
    "        #actonclimate       0.92      0.70      0.80       568\n",
    "           #bushfires       0.91      0.77      0.83       270\n",
    "       #climateaction       0.91      0.64      0.75      1913\n",
    "       #climatechange       0.81      0.90      0.85      4857\n",
    "       #climatecrisis       0.89      0.70      0.78      1495\n",
    "       #climatestrike       0.84      0.96      0.89      5507\n",
    "       #environment         0.86      0.85      0.86      1411\n",
    "       #fridaysforfuture    0.97      0.46      0.62       911\n",
    "       #globalwarming       0.82      0.87      0.84      1246\n",
    "        #greennewdeal       0.87      0.91      0.89      1377\n",
    "       #savetheplanet       0.94      0.78      0.85       430\n",
    "      #sustainability       0.88      0.97      0.92      1737\n",
    "\n",
    "         accuracy                           0.85     21722\n",
    "        macro avg       0.89      0.79      0.82     21722\n",
    "     weighted avg       0.86      0.85      0.85     21722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8513028266273823"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is --- 0.8101003590829574 \n",
    "Accuracy changed to point 0.85 after tuning the LinearSVC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitterLinearSVCModel.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "joblib.dump(text_clf, 'twitterLinearSVCModel.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['#environment', '#climatechange', '#climatechange',\n",
       "       '#climatestrike'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"Cuyahoga county eliminates use of plastic bags\",\n",
    "                  \"Weather is still warm in winters and is not freezing\",\n",
    "                  \"Actions are more important for a greater cause\",\n",
    "                  \"Its freezing out today may be need to drive my SUV more\"\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#climatechange'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"Too much rain in moonsoon\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#climatestrike'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"Schools strikes in sweden\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#climatechange'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"buy a tesla save the earth\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tasne\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tasne\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['#actonclimate', '#bushfiresaustralia', '#climateaction', '#fridaysforfuture', '#greennewdeal', \"'\", 'able', 'across', \"ain't\", 'almost', 'also', 'among', \"can't\", 'cannot', \"could've\", 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', \"how'd\", \"how'll\", 'however', 'least', 'let', 'like', 'likely', 'may', 'might', \"might've\", 'must', \"must've\", 'neither', 'often', 'rather', 'said', 'say', 'says', 'since', 'tis', 'twas', 'us', 'wants', \"what'd\", \"when'd\", \"when'll\", \"where'd\", \"where'll\", \"who'd\", \"who'll\", \"why'd\", \"why'll\", \"would've\", 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.86\n",
      "Best parameters:  {'clf__C': 0.01, 'tfidf__ngram_range': (1, 1), 'tfidf__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_svm = {'tfidf__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False),'clf__C': (0.001, 0.01, 0.1, 1)}\n",
    "gs_clf = GridSearchCV(text_clf, parameters_svm, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(gs_clf.best_score_))\n",
    "print(\"Best parameters: \", gs_clf.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
