{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_location = pd.read_csv('../data/graphAnalysis/clean_climateTwitterData.csv')\n",
    "tweet_df_location = tweet_df_location[tweet_df_location['text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# This step already done in Sentiment Analyzer\n",
    "# def replace_urls(in_string, replacement=None):\n",
    "#     # \"\"\"Replace URLs in strings. See also: ``bit.ly/PyURLre``\n",
    "\n",
    "#     # Args:\n",
    "#     #     in_string (str): string to filter\n",
    "#     #     replacement (str or None): replacment text. defaults to '<-URL->'\n",
    "\n",
    "#     # Returns:\n",
    "#     #     str\n",
    "#     # \"\"\"\n",
    "#     replacement = '<-URL->' if replacement is None else replacement\n",
    "#     pattern = re.compile('(https?://)?(\\w*[.]\\w+)+([/?=&]+\\w+)*')\n",
    "#     return re.sub(pattern, replacement, in_string)\n",
    "\n",
    "\n",
    "def tokenize_only(in_string):\n",
    "    \"\"\"\n",
    "    Convert `in_string` of text to a list of tokens using NLTK's TweetTokenizer\n",
    "    \"\"\"\n",
    "    # reasonable, but adjustable tokenizer settings\n",
    "    tokenizer = TweetTokenizer(preserve_case=False,\n",
    "                               reduce_len=True,\n",
    "                               strip_handles=False)\n",
    "    tokens = tokenizer.tokenize(in_string)\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step already done in Sentiment Analyzer\n",
    "# # remove urls and retweets and entities from the text\n",
    "# tweet_df_location['text_clean'] = tweet_df_location['text'].apply(lambda row:replace_urls(row))\n",
    "\n",
    "# #remove punctuations\n",
    "# RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])  \n",
    "# tweet_df_location['text_clean'] = tweet_df_location['text_clean'].str.replace(RE_PUNCTUATION, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"you'll\", 'the', 'o', 'just', \"where's\", 'wasn', \"he'd\", ' its', ' says', 'y', 'up', ' wants', 'then', ' he', 'wouldn', 'our', ' them', 'after', ' there', 'should', 'shouldn', 'i', \"didn't\", \"hasn't\", 'll', 'so', \" should've\", ' but', ' else', \" couldn't\", ' in', 'its', \"let's\", ' other', ' said', \" i've\", \"it's\", 'on', \"he's\", \"you've\", 'further', '#GreenNewDeal', '#sustainability', 'before', ' also', \"why's\", 'could', ' his', \" why's\", \" might've\", 'too', ' neither', ' nor', \" i'll\", ' did', \" don't\", 'between', 'once', ' she', '#globalwarming', 'their', 'few', ' ever', \"'tis\", '#environment', \"mightn't\", 'hasn', \" they'd\", ' of', ' should', ' must', ' us', 'they', ' likely', 'myself', 'can', \" you'd\", 'by', 'didn', \"we're\", ' were', ' had', ' every', \" he's\", 'other', 'which', \" what's\", 'is', 'from', \" mustn't\", ' and', ' an', ' with', \" we'd\", 'aren', ' too', ' no', 'http', \" can't\", 'most', 'her', ' her', ' or', 'my', 'under', ' hers', \" it's\", 'over', '#climateStrike', \"i've\", \" ain't\", 'doesn', '#FridaysForFuture', \"we'll\", 'has', \" would've\", \"aren't\", \" who's\", 'those', 'it', \"there's\", ' into', \" where's\", \" you're\", ' than', ' rather', 'she', 'ours', 't', \"shan't\", \" won't\", ' own', \" how'd\", ' whom', 'are', \"he'll\", ' on', '#climatestrike', 'am', 'own', 'had', 'each', 'yourselves', \" aren't\", \" he'd\", ' our', ' my', '#ActOnClimate', 'through', 'weren', 'that', ' these', ' by', 'to', ' able', \"weren't\", 'itself', ' have', \" how's\", ' the', \" shan't\", \"she'd\", 'as', \" what'd\", ' from', 'will', 'having', ' any', \" when'll\", ' yet', 'for', 'd', ' least', 'did', \"shouldn't\", 'being', \"hadn't\", 'm', \" he'll\", 'himself', \" didn't\", \" they're\", ' would', ' may', 'hadn', \" when's\", ' their', 'we', \"mustn't\", \" doesn't\", ' they', \"they've\", ' this', \" who'd\", 'be', 'until', ' which', ' however', 'into', \"wasn't\", '#GlobalWarming', \" must've\", 'needn', \"couldn't\", ' i', 'why', ' got', 'such', 'them', \"they'll\", ' am', 'all', 'hers', \"doesn't\", ' almost', ' are', 'this', 'do', ' what', 'have', \"here's\", '#climateAction', 'or', \" they've\", \" why'd\", \"i'll\", \" she's\", \"we'd\", 'same', \" shouldn't\", ' because', \" there's\", \" isn't\", 'isn', \"won't\", \" who'll\", '#climatechange', 'shan', 'couldn', 'was', 'an', ' been', 'in', \"haven't\", 'he', 'yours', ' it', ' tis', ' him', 'doing', 'with', 'no', \" could've\", 'below', 'because', 'down', \" wouldn't\", ' where', ' be', 'were', 'your', ' not', \" we'll\", \" hasn't\", ' while', ' why', 'not', ' to', 'ourselves', \"who's\", 'mightn', 'about', ' after', 'during', 'herself', 'but', 'ain', 've', \"that's\", ' just', 'what', \" she'd\", \" how'll\", \" i'm\", \"when's\", ' will', \"they'd\", 'does', 'and', \"i'd\", ' like', \" they'll\", ' your', ' twas', \" that'll\", ' since', \"you'd\", 'off', 'above', \"should've\", ' that', ' either', 'been', \" where'll\", 'of', '#climatecrisis', 'these', 'whom', 'where', ' off', 'him', 'who', ' we', \" when'd\", ' for', \" weren't\", \"needn't\", 'when', ' how', \"they're\", ' among', ' only', 'against', 'don', \"we've\", \" mightn't\", 'both', \" we're\", 'than', ' cannot', \"i'm\", '#bushfires', 'ought', ' does', 'yourself', ' say', \"isn't\", \"how's\", \"she'll\", ' across', 'themselves', 'again', ' who', 'haven', 'nor', \" 'twas\", 'now', \"what's\", 'any', ' do', 'ma', 'https', 'some', ' about', 'only', \" she'll\", \" you've\", 'very', ' could', ' often', ' if', 'while', '#savetheplanet', 'there', \"that'll\", 'a', 'his', 's', ' you', '#bushfiresAustralia', ' get', \"don't\", 'me', ' me', 're', 'won', 'here', \" that's\", ' let', ' might', \" you'll\", \" why'll\", 'out', \" i'd\", ' is', 'how', 'more', 'at', ' a', \" where'd\", ' most', ' so', ' has', \" wasn't\", ' as', 'would', ' some', ' all', 'you', ' at', \"she's\", ' was', \"wouldn't\", ' dear', ' when', 'mustn', ' then', ' can', \"you're\", 'if', 'theirs']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# List of stopwords\n",
    "stop_words= stopwords.words('english') #import stopwords from NLTK package\n",
    "readInStopwords = pd.read_csv(\"pre_process/twitterStopWords.csv\", encoding='ISO-8859-1') # import stopwords from CSV file as pandas data frame\n",
    "readInStopwords = readInStopwords.wordList.tolist() # convert pandas data frame to a list\n",
    "readInStopwords.append('http')\n",
    "readInStopwords.append('https')\n",
    "\n",
    "search_terms = ['#climateStrike','#climatestrike','#climatechange','#GreenNewDeal','#climatecrisis','#climateAction','#FridaysForFuture',\n",
    "            '#environment','#globalwarming','#GlobalWarming','#ActOnClimate','#sustainability','#savetheplanet',\n",
    "        '#bushfiresAustralia','#bushfires']\n",
    "readInStopwords.extend(search_terms)\n",
    "stop_list = stop_words + readInStopwords # combine two lists i.e. NLTK stop words and CSV stopwords\n",
    "stop_list = list(set(stop_list)) # strore only unique values \n",
    "    \n",
    "print(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweet_df_location['text_clean']\n",
    "y = tweet_df_location['search_hashtags']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tasne\\envs\\newproject\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['#actonclimate', '#bushfiresaustralia', '#climateaction', '#fridaysforfuture', '#greennewdeal', \"'\", 'able', 'across', \"ain't\", 'almost', 'also', 'among', \"can't\", 'cannot', \"could've\", 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', \"how'd\", \"how'll\", 'however', 'least', 'let', 'like', 'likely', 'may', 'might', \"might've\", 'must', \"must've\", 'neither', 'often', 'rather', 'said', 'say', 'says', 'since', 'tis', 'twas', 'us', 'wants', \"what'd\", \"when'd\", \"when'll\", \"where'd\", \"where'll\", \"who'd\", \"who'll\", \"why'd\", \"why'll\", \"would've\", 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.00, stop_words=stop_list, tokenizer=tokenize_only) # Use tf (raw term count) features for LDA.\n",
    "X_Vect = tf_vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_Vect, y, test_size=0.30, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50683, 87818)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50683,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21722, 87818)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21722,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RFC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7970720928091336 -- Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strVal = [\"People are getting aware of the surrounding \",\"things are super bad\"]\n",
    "Value = text_Vect.transform(strVal)\n",
    "Value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.predict(Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
