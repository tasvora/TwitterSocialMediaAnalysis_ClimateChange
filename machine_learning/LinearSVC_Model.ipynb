{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_location = pd.read_csv('../data/graphAnalysis/clean_climateTwitterData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72405"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_df_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_location = tweet_df_location[tweet_df_location['text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72405"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_df_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "search_hashtags\n",
       "#actonclimate         1895\n",
       "#bushfires             899\n",
       "#climateaction        6378\n",
       "#climatechange       16190\n",
       "#climatecrisis        4982\n",
       "#climatestrike       18355\n",
       "#environment          4703\n",
       "#fridaysforfuture     3038\n",
       "#globalwarming        4152\n",
       "#greennewdeal         4589\n",
       "#savetheplanet        1434\n",
       "#sustainability       5790\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_location.groupby('search_hashtags')['id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# This step was performed in the sentiment Analyzer\n",
    "# def replace_urls(in_string, replacement=None):\n",
    "#     # \"\"\"Replace URLs in strings. See also: ``bit.ly/PyURLre``\n",
    "\n",
    "#     # Args:\n",
    "#     #     in_string (str): string to filter\n",
    "#     #     replacement (str or None): replacment text. defaults to '<-URL->'\n",
    "\n",
    "#     # Returns:\n",
    "#     #     str\n",
    "#     # \"\"\"\n",
    "#     replacement = '<-URL->' if replacement is None else replacement\n",
    "#     pattern = re.compile('(https?://)?(\\w*[.]\\w+)+([/?=&]+\\w+)*')\n",
    "#     return re.sub(pattern, replacement, in_string)\n",
    "\n",
    "\n",
    "def tokenize_only(in_string):\n",
    "    \"\"\"\n",
    "    Convert `in_string` of text to a list of tokens using NLTK's TweetTokenizer\n",
    "    \"\"\"\n",
    "    # reasonable, but adjustable tokenizer settings\n",
    "    tokenizer = TweetTokenizer(preserve_case=False,\n",
    "                               reduce_len=True,\n",
    "                               strip_handles=False)\n",
    "    tokens = tokenizer.tokenize(in_string)\n",
    "    return tokens\n",
    "\n",
    "# #remove punctuations\n",
    "# def rm_punctuation(tweet):\n",
    "#     RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])  \n",
    "#     print(f'this is the punctuation  {RE_PUNCTUATION}')\n",
    "#     print(\"\\n\")\n",
    "#     newValue = tweet.replace(RE_PUNCTUATION, \"\")\n",
    "#     return newValue\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step was performed in the sentiment Analyzer\n",
    "# # remove urls and retweets and entities from the text\n",
    "# tweet_df_location['text_clean'] = tweet_df_location['text'].apply(lambda row:replace_urls(row))\n",
    "\n",
    "# #remove punctuations\n",
    "# RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])  \n",
    "# tweet_df_location['text_clean'] = tweet_df_location['text_clean'].str.replace(RE_PUNCTUATION, \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'isn', ' what', \" would've\", \" when'll\", ' of', '#climatecrisis', 'yours', ' in', 'above', ' however', ' so', ' some', ' did', 'were', 'then', \"didn't\", \" ain't\", 'those', 'y', ' this', 'only', \" he'll\", 'and', \" they're\", 'the', 'had', 'which', 'is', \" we're\", \" they'd\", ' any', ' your', 'to', \" who's\", 'can', 'her', 'more', ' its', ' these', ' a', 'be', 'such', \" when'd\", \" she'll\", 'hadn', ' be', 'll', \" we'll\", '#climatestrike', ' while', 'theirs', \"i've\", \"you'd\", 'both', 'most', ' at', 'himself', \" couldn't\", \"should've\", ' should', 'during', ' than', \"mightn't\", \"we're\", ' across', \" what'd\", ' been', \" might've\", ' said', \" we'd\", 'could', 'ain', ' other', \"'tis\", '#GreenNewDeal', ' from', \" wouldn't\", ' him', 'our', \"when's\", \" mightn't\", 'yourselves', \" aren't\", '#sustainability', \" weren't\", ' cannot', 'hers', \" he'd\", ' on', ' with', ' let', \"you've\", \"there's\", ' is', \" he's\", 'its', 'of', \"you'll\", 'weren', 'https', 'that', 's', 'over', ' neither', 'shouldn', \" you're\", ' most', ' she', ' then', 'nor', 'after', ' as', 'if', '#bushfires', ' able', ' must', \" she's\", \" didn't\", 'have', ' because', ' which', ' get', 'needn', 'shan', 'been', ' got', 'doesn', \"where's\", ' who', \" why's\", ' her', ' too', 'i', 'ourselves', 'this', \" should've\", 'http', 'are', \"i'd\", ' an', ' was', 'further', 'you', ' also', ' how', 'should', ' they', 'each', \" how'd\", \" there's\", ' either', \" could've\", \" where'll\", ' and', 'doing', 'under', 'through', \" can't\", 'themselves', 'while', \"he's\", \" hasn't\", 'she', \"hasn't\", ' that', \" how'll\", ' are', 'from', \" i'd\", ' me', 'all', ' you', ' does', ' why', 'because', \"don't\", 'was', ' tis', ' has', 'having', ' by', ' least', ' only', \"mustn't\", 'your', \"they'd\", \"they'll\", ' rather', ' twas', ' hers', \" wasn't\", 'very', 'about', 'mustn', 'there', \" i've\", \"here's\", ' can', \" that'll\", 'so', '#environment', 'has', 'as', \" why'll\", 'itself', 'a', 'didn', 'who', 'by', \" won't\", ' like', 'them', ' no', 'but', ' them', 'm', \" they've\", 'they', 'wasn', 'any', 'he', \" how's\", 'on', \" 'twas\", 'own', \"she'd\", ' us', \"who's\", 'me', 'does', 'here', \" where'd\", ' say', \"isn't\", 'yourself', \"she'll\", ' but', ' into', \" mustn't\", 'too', '#globalwarming', ' will', 'these', ' likely', ' their', 'do', 'would', 'again', \"he'd\", 'their', \" why'd\", \"we've\", 'before', ' may', ' if', \" it's\", 'couldn', 'not', \"hadn't\", \"that's\", 'below', 'with', 'no', ' wants', 'some', \"we'd\", \"won't\", 'or', 'myself', 'will', ' says', 'don', '#savetheplanet', \" you'd\", ' all', 'out', ' else', \"couldn't\", 'd', \"why's\", \" isn't\", ' might', 'what', \" shan't\", \" what's\", ' dear', 'just', 'his', ' when', \" you'll\", ' do', ' off', ' i', 'off', \" who'll\", '#GlobalWarming', 'him', 'up', \" shouldn't\", \"aren't\", \"wasn't\", 'than', ' ever', 'other', ' where', \"they're\", 'once', \" must've\", ' have', ' almost', 'it', \"he'll\", ' every', \" doesn't\", 'won', ' there', \"doesn't\", 'until', 'wouldn', 'between', \" don't\", \" when's\", ' nor', 'where', 'now', 'at', ' the', \"haven't\", 'did', ' would', '#climateAction', \"they've\", 'haven', '#bushfiresAustralia', ' since', 'how', ' or', \" that's\", '#ActOnClimate', 'same', ' yet', 'an', 'few', 're', ' whom', ' had', \"she's\", \" who'd\", \"needn't\", ' it', 'my', 'against', 'hasn', 'o', \"let's\", ' to', 'whom', \" you've\", ' for', '#climateStrike', 'ours', ' he', ' just', 'mightn', \"i'm\", ' often', \" they'll\", 'ma', \"wouldn't\", ' could', 'ought', ' his', 'when', ' we', ' among', 'down', 've', \"shan't\", 'we', 'being', \"weren't\", ' were', 'into', \"how's\", \"that'll\", \" i'm\", 'aren', 't', 'herself', ' about', '#FridaysForFuture', \"you're\", \"i'll\", '#climatechange', \"we'll\", ' not', ' am', ' our', \"it's\", \"what's\", 'in', \"shouldn't\", ' own', \" she'd\", ' after', \" i'll\", \" where's\", 'why', ' my', 'for']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# List of stopwords\n",
    "stop_words= stopwords.words('english') #import stopwords from NLTK package\n",
    "readInStopwords = pd.read_csv(\"pre_process/twitterStopWords.csv\", encoding='ISO-8859-1') # import stopwords from CSV file as pandas data frame\n",
    "readInStopwords = readInStopwords.wordList.tolist() # convert pandas data frame to a list\n",
    "\n",
    "readInStopwords.append('http')\n",
    "readInStopwords.append('https')\n",
    "\n",
    "search_terms = ['#climateStrike','#climatestrike','#climatechange','#GreenNewDeal','#climatecrisis','#climateAction','#FridaysForFuture',\n",
    "            '#environment','#globalwarming','#GlobalWarming','#ActOnClimate','#sustainability','#savetheplanet',\n",
    "        '#bushfiresAustralia','#bushfires']\n",
    "\n",
    "readInStopwords.extend(search_terms)\n",
    "stop_list = stop_words + readInStopwords # combine two lists i.e. NLTK stop words and CSV stopwords\n",
    "stop_list = list(set(stop_list)) # strore only unique values \n",
    "    \n",
    "print(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweet_df_location['text_clean']\n",
    "y = tweet_df_location['search_hashtags']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder / Not Required\n",
    "\n",
    "# # Step 1: Label-encode data set\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.fit(y)\n",
    "# encoded_y = label_encoder.transform(y)\n",
    "# encoded_y.shape\n",
    "# encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tasne\\envs\\newproject\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['#actonclimate', '#bushfiresaustralia', '#climateaction', '#fridaysforfuture', '#greennewdeal', \"'\", 'able', 'across', \"ain't\", 'almost', 'also', 'among', \"can't\", 'cannot', \"could've\", 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', \"how'd\", \"how'll\", 'however', 'least', 'let', 'like', 'likely', 'may', 'might', \"might've\", 'must', \"must've\", 'neither', 'often', 'rather', 'said', 'say', 'says', 'since', 'tis', 'twas', 'us', 'wants', \"what'd\", \"when'd\", \"when'll\", \"where'd\", \"where'll\", \"who'd\", \"who'll\", \"why'd\", \"why'll\", \"would've\", 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.00, stop_words=stop_list, tokenizer=tokenize_only) # Use tf (raw term count) features for LDA.\n",
    "X_Vect = tf_vectorizer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72405, 87818)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_Vect, y, test_size=0.30, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50683, 87818)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50683,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21722, 87818)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21722,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC()\n",
    "svm.fit(X_train, y_train)  \n",
    "predictions = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 420    1   17   40   26   30    5    6    1   15    0    7]\n",
      " [   0  226    2   29    6    1    2    1    3    0    0    0]\n",
      " [  15    3 1213  222   61  241   29   42   26   29    7   25]\n",
      " [  37   18  211 3828  141  247   84   29  131   59   22   50]\n",
      " [  16    9   78  156 1009  118   17   13   27   41    4    7]\n",
      " [  15    5   96  135   56 4986   16  107   27   41    9   14]\n",
      " [   9    0   13   64   10   14 1189    1   27    3   10   71]\n",
      " [   4    3   35   31   10  318    5  483    6    4    6    6]\n",
      " [   2    0    5   65   18   33   18    1 1089    5    5    5]\n",
      " [  14    0   13   55   27   40    4    7    6 1199    6    6]\n",
      " [   1    0    0   10    5    7   20    1   14    0  357   15]\n",
      " [   4    0   11   38    8    8   60    5   19    3   12 1569]]\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    #actonclimate       0.78      0.74      0.76       568\n",
      "       #bushfires       0.85      0.84      0.84       270\n",
      "   #climateaction       0.72      0.63      0.67      1913\n",
      "   #climatechange       0.82      0.79      0.80      4857\n",
      "   #climatecrisis       0.73      0.67      0.70      1495\n",
      "   #climatestrike       0.83      0.91      0.86      5507\n",
      "     #environment       0.82      0.84      0.83      1411\n",
      "#fridaysforfuture       0.69      0.53      0.60       911\n",
      "   #globalwarming       0.79      0.87      0.83      1246\n",
      "    #greennewdeal       0.86      0.87      0.86      1377\n",
      "   #savetheplanet       0.82      0.83      0.82       430\n",
      "  #sustainability       0.88      0.90      0.89      1737\n",
      "\n",
      "         accuracy                           0.81     21722\n",
      "        macro avg       0.80      0.79      0.79     21722\n",
      "     weighted avg       0.81      0.81      0.81     21722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8087653070619648"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy is -- 0.8101924316361293 -- without tokens and stopwords\n",
    "- Accuracy is -- 0.8087653070619648 -- with tokenization and stopwords\n",
    "> Note adding or remove tokens and stopwords does not effect  accuracy much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 87818)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strVal = [\"Cuyahoga county eliminates use of plastic bags\",\"Weather is still warm in winters and is not freezing\"]\n",
    "Value = tf_vectorizer.transform(strVal)\n",
    "Value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svm.predict(Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#environment' '#climatechange']\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this way to obtain the value from encoding\n",
    "# inv_s = label_encoder.inverse_transform(predictions.astype(int).ravel())\n",
    "# inv_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.84\n",
      "Best parameters:  {'C': 0.01}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1]}\n",
    "grid = GridSearchCV(LinearSVC(), param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best cross-validation score: 0.84\n",
    "- Best parameters:  {'C': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['#environment', '#climatechange'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.predict(Value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
