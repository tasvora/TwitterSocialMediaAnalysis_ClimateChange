{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_location = pd.read_csv('../data/graphAnalysis/clean_climateTwitterData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72405"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_df_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_location = tweet_df_location[tweet_df_location['text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72405"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_df_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "search_hashtags\n",
       "#actonclimate         1895\n",
       "#bushfires             899\n",
       "#climateaction        6378\n",
       "#climatechange       16190\n",
       "#climatecrisis        4982\n",
       "#climatestrike       18355\n",
       "#environment          4703\n",
       "#fridaysforfuture     3038\n",
       "#globalwarming        4152\n",
       "#greennewdeal         4589\n",
       "#savetheplanet        1434\n",
       "#sustainability       5790\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df_location.groupby('search_hashtags')['id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# This step was performed in the sentiment Analyzer\n",
    "# def replace_urls(in_string, replacement=None):\n",
    "#     # \"\"\"Replace URLs in strings. See also: ``bit.ly/PyURLre``\n",
    "\n",
    "#     # Args:\n",
    "#     #     in_string (str): string to filter\n",
    "#     #     replacement (str or None): replacment text. defaults to '<-URL->'\n",
    "\n",
    "#     # Returns:\n",
    "#     #     str\n",
    "#     # \"\"\"\n",
    "#     replacement = '<-URL->' if replacement is None else replacement\n",
    "#     pattern = re.compile('(https?://)?(\\w*[.]\\w+)+([/?=&]+\\w+)*')\n",
    "#     return re.sub(pattern, replacement, in_string)\n",
    "\n",
    "\n",
    "def tokenize_only(in_string):\n",
    "    \"\"\"\n",
    "    Convert `in_string` of text to a list of tokens using NLTK's TweetTokenizer\n",
    "    \"\"\"\n",
    "    # reasonable, but adjustable tokenizer settings\n",
    "    tokenizer = TweetTokenizer(preserve_case=False,\n",
    "                               reduce_len=True,\n",
    "                               strip_handles=False)\n",
    "    tokens = tokenizer.tokenize(in_string)\n",
    "    return tokens\n",
    "\n",
    "# #remove punctuations\n",
    "# def rm_punctuation(tweet):\n",
    "#     RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])  \n",
    "#     print(f'this is the punctuation  {RE_PUNCTUATION}')\n",
    "#     print(\"\\n\")\n",
    "#     newValue = tweet.replace(RE_PUNCTUATION, \"\")\n",
    "#     return newValue\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2020 is the year we votethemout the year we cl...\n",
       "1        winter has not stopped this group of dedicated...\n",
       "2        week 55 of climatestrike at the un next week f...\n",
       "3         a year of resistance as youth protests shaped...\n",
       "4         happy holidays greta gretathunberg climatecha...\n",
       "5         10 questions to ask politicians about climate...\n",
       "6        climatestrike fridaysforfuture portraits uniqu...\n",
       "7        climatechangeisreal climatestrike climateactio...\n",
       "8        my oldest daughter finding inspiration and enc...\n",
       "9        our toddler potus whined this week about time ...\n",
       "10       the change is going to come from the people de...\n",
       "11       moments after impeachmentvote in judiciary com...\n",
       "12       climatestrike climatechangeisreal climatecrisi...\n",
       "13       keep up the great work be safe and stay away f...\n",
       "14       congratulations gretathunberg for inspiring pe...\n",
       "15       even though i hoped hongkongprotesters would w...\n",
       "16        gretathunberg is the youngest time personofth...\n",
       "17        congratulations to inspiring environmental ac...\n",
       "18       i get my energy and hope by organizing with yo...\n",
       "19        the chamber of commerce mised them taking the...\n",
       "20       while police protect wall st banks funding cli...\n",
       "21       individual behavior will help make the differe...\n",
       "22       whywestrike peace on earth climatestrike frida...\n",
       "23       hocomd climatestrike greennewdeal climatecrisi...\n",
       "24       sunrisebmore sunrisemvmt climatestrike greenne...\n",
       "25       painfully appropriate to see this message on l...\n",
       "26       the climatestrike event in baltimore had heavy...\n",
       "27       we’ll be back wellsfargo climatestrike greenpe...\n",
       "28       the activists are still locked down at wellsfa...\n",
       "29       climatestrike outside wells fargo upstairs at ...\n",
       "                               ...                        \n",
       "72375        changetheworld lovethyneighbor savetheplanet \n",
       "72376    finished up our earthday projects on this glob...\n",
       "72377    i can’t stand this so much work during the 60s...\n",
       "72378    when the sun comes out after a storm its a bea...\n",
       "72379    just bought a big gulp with my refill cup and ...\n",
       "72380    wouldn’t it be cool if some day every single h...\n",
       "72381    happy earth day recylcle happyearthday turnove...\n",
       "72382    paperwork doesnt just hurt productivity it kil...\n",
       "72383    had a wonderful time today for zenithgallerydc...\n",
       "72384    governments around the globe earth day should ...\n",
       "72385    lildickytweets where this money going i dig yo...\n",
       "72386    straws suck savetheplanet zekes beans amp bowl...\n",
       "72387    keep the earth in tune our new savetheplanet t...\n",
       "72388                                       savetheplanet \n",
       "72389    saving the planet one shopping cart weird blob...\n",
       "72390                          savetheplanet saveouroceans\n",
       "72391    nope no climatecrisis happening here but the c...\n",
       "72392                           saveanimals savetheplanet \n",
       "72393    start a trend agam act generous and magnanimou...\n",
       "72394    if any of my followers are ticked for the rt o...\n",
       "72395    heart breaking stopusing plastics savethewhale...\n",
       "72396    it’s not beef but cow with lovely big eyes it’...\n",
       "72397    how will sea level change slc affect maryland ...\n",
       "72398    endplasticwaste savetheplanet can we just stop...\n",
       "72399          always feared this recycling savetheplanet \n",
       "72400    no more straws at lbm only if you ask for it y...\n",
       "72401    my trumps may not believe in climatechange but...\n",
       "72402    time is over act now friday4future climatechan...\n",
       "72403    this is my first contribution on visualizing t...\n",
       "72404    this assault on science is an outrage and shou...\n",
       "Name: text_clean, Length: 72405, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This step was performed in the sentiment Analyzer\n",
    "# # remove urls and retweets and entities from the text\n",
    "# tweet_df_location['text_clean'] = tweet_df_location['text'].apply(lambda row:replace_urls(row))\n",
    "\n",
    "# #remove punctuations\n",
    "# RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])  \n",
    "# tweet_df_location['text_clean'] = tweet_df_location['text_clean'].str.replace(RE_PUNCTUATION, \"\")\n",
    "\n",
    "tweet_df_location['text_clean'] = tweet_df_location['text_clean'].str.lower()\n",
    "tweet_df_location['text_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" don't\", \" 'twas\", 'wouldn', 'between', ' where', ' but', \"should've\", ' say', \" where's\", 'doesn', \"she'll\", 'will', ' either', ' almost', \" he'll\", ' no', \" who'd\", ' into', 'you', \" weren't\", \"i'd\", 'have', \" where'd\", \" can't\", 'don', \" shan't\", \"i've\", ' me', 'y', 'of', 'each', \"haven't\", ' could', \"when's\", 'is', ' whom', 'did', 'up', \" she'll\", 'to', 'an', \"didn't\", \" you're\", 'am', '#climatestrike', ' ever', ' there', \"it's\", 'then', \"that's\", ' had', ' this', \" mightn't\", 'hadn', \"needn't\", 'his', ' than', 'myself', \"they'd\", ' that', ' else', \" when'd\", \"aren't\", \" they're\", \"you've\", ' after', ' should', 'above', \"isn't\", ' them', \"wasn't\", \"where's\", \"he'll\", 'here', 've', '#climateAction', 'ought', 'that', ' cannot', 'own', ' she', 'at', ' been', 'again', \"won't\", '#climatecrisis', 'can', 'hasn', 'these', 'yours', \" they've\", 'the', ' we', \"wouldn't\", ' in', \"why's\", ' which', ' likely', 'him', ' he', ' often', ' an', \"shan't\", ' a', \"who's\", \"how's\", 'nor', \" must've\", ' might', ' too', ' how', ' if', 'until', \" it's\", 'other', \"there's\", 'won', \"you'd\", 'but', ' among', ' it', '#environment', ' own', 'they', ' able', 'yourself', '#bushfires', \" she'd\", ' neither', ' while', 'has', 'who', 'haven', \"doesn't\", \" i'm\", 'through', \"we'd\", 'most', 'ourselves', ' him', ' be', \"that'll\", \" there's\", \"they've\", ' has', ' then', ' us', 'only', ' were', ' or', 'should', \" that'll\", \" who'll\", 'now', ' across', ' rather', 'all', 'those', ' with', 'd', 'how', 'll', 'be', 'so', 'against', 'it', 'being', 't', '#climateStrike', ' only', 'i', 'some', ' just', ' these', ' also', 'himself', 'yourselves', 'having', \"'tis\", 'into', \"shouldn't\", 'with', 'been', \"you're\", \"you'll\", ' of', \"here's\", \" you've\", \" who's\", 'off', ' like', 'where', ' will', \" wouldn't\", ' because', ' every', ' at', \"what's\", 'could', ' on', 'than', ' why', \" how'd\", \"we've\", ' i', 'whom', \" i've\", 'which', \" why'd\", 'https', 'as', 'didn', ' any', ' for', 'when', 'shan', \" how's\", '#GreenNewDeal', ' nor', ' twas', ' get', ' so', 'this', 'them', '#bushfiresAustralia', ' some', \" he's\", ' says', ' do', 'very', 'why', \" should've\", \" where'll\", 'shouldn', '#sustainability', \"i'm\", ' they', 'too', 're', ' who', 'down', \" why's\", ' does', \"hadn't\", \"she's\", ' am', 'or', ' their', 'a', 'mightn', \" could've\", \" wasn't\", ' as', \"we're\", \" we'll\", 'would', ' other', 'was', ' dear', 'our', 'from', \" why'll\", ' would', \" she's\", 'if', ' let', \" mustn't\", ' the', \" what's\", ' from', 'were', 'after', 'no', 'its', '#globalwarming', \"they'll\", \" you'll\", 'under', ' hers', ' said', \"let's\", 'once', ' however', \" hasn't\", 'further', \" won't\", ' wants', \" couldn't\", 'your', 'before', 'more', 'by', 'there', 'same', \"hasn't\", 'do', 'on', ' his', \"i'll\", \" that's\", 'm', 'just', '#climatechange', \" isn't\", \" when's\", 'doing', 'needn', 'o', 'hers', ' when', \" i'd\", \" you'd\", \"mustn't\", 'over', 'mustn', 'does', 'are', 'her', 'out', 'not', ' was', 'themselves', 'he', \"he'd\", 'such', ' you', 'wasn', ' and', \" they'll\", \" shouldn't\", \"weren't\", ' to', \" when'll\", \"they're\", 'and', 'what', ' have', 'ours', ' its', ' off', ' did', \" he'd\", ' by', ' your', \"don't\", ' got', 'herself', \" might've\", \" ain't\", \"she'd\", 'because', '#ActOnClimate', \" i'll\", 'http', 'weren', '#FridaysForFuture', \"he's\", ' least', 'while', \" didn't\", ' must', ' yet', '#GlobalWarming', 'below', ' tis', 'we', \" they'd\", ' not', 'ain', 'my', 'few', \"mightn't\", '#savetheplanet', 'during', ' her', ' can', 'she', ' may', 'isn', 'for', ' our', \" we'd\", 'their', 'theirs', ' all', 'any', 'ma', 'both', 'in', ' are', \" doesn't\", \" would've\", 'about', \" what'd\", 'couldn', ' what', ' about', ' is', 'had', \"couldn't\", ' since', \" how'll\", 's', 'aren', \" we're\", \"we'll\", 'me', ' most', 'itself', ' my', \" aren't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# List of stopwords\n",
    "stop_words= stopwords.words('english') #import stopwords from NLTK package\n",
    "readInStopwords = pd.read_csv(\"pre_process/twitterStopWords.csv\", encoding='ISO-8859-1') # import stopwords from CSV file as pandas data frame\n",
    "readInStopwords = readInStopwords.wordList.tolist() # convert pandas data frame to a list\n",
    "\n",
    "readInStopwords.append('http')\n",
    "readInStopwords.append('https')\n",
    "\n",
    "search_terms = ['#climateStrike','#climatestrike','#climatechange','#GreenNewDeal','#climatecrisis','#climateAction','#FridaysForFuture',\n",
    "            '#environment','#globalwarming','#GlobalWarming','#ActOnClimate','#sustainability','#savetheplanet',\n",
    "        '#bushfiresAustralia','#bushfires']\n",
    "\n",
    "readInStopwords.extend(search_terms)\n",
    "stop_list = stop_words + readInStopwords # combine two lists i.e. NLTK stop words and CSV stopwords\n",
    "stop_list = list(set(stop_list)) # strore only unique values \n",
    "    \n",
    "print(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweet_df_location['text_clean']\n",
    "y = tweet_df_location['search_hashtags']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder / Not Required\n",
    "\n",
    "# # Step 1: Label-encode data set\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.fit(y)\n",
    "# encoded_y = label_encoder.transform(y)\n",
    "# encoded_y.shape\n",
    "# encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tasne\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['#actonclimate', '#bushfiresaustralia', '#climateaction', '#fridaysforfuture', '#greennewdeal', \"'\", 'able', 'across', \"ain't\", 'almost', 'also', 'among', \"can't\", 'cannot', \"could've\", 'dear', 'either', 'else', 'ever', 'every', 'get', 'got', \"how'd\", \"how'll\", 'however', 'least', 'let', 'like', 'likely', 'may', 'might', \"might've\", 'must', \"must've\", 'neither', 'often', 'rather', 'said', 'say', 'says', 'since', 'tis', 'twas', 'us', 'wants', \"what'd\", \"when'd\", \"when'll\", \"where'd\", \"where'll\", \"who'd\", \"who'll\", \"why'd\", \"why'll\", \"would've\", 'yet'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.00, stop_words=stop_list, tokenizer=tokenize_only) # Use tf (raw term count) features for LDA.\n",
    "X_Vect = tf_vectorizer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72405, 87818)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_Vect, y, test_size=0.30, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50683, 87818)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50683,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21722, 87818)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21722,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC()\n",
    "svm.fit(X_train, y_train)  \n",
    "predictions = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 420    1   17   40   26   30    5    6    1   15    0    7]\n",
      " [   0  226    2   29    6    1    2    1    3    0    0    0]\n",
      " [  15    3 1213  222   61  241   29   42   26   29    7   25]\n",
      " [  37   18  211 3828  141  247   84   29  131   59   22   50]\n",
      " [  16    9   78  156 1009  118   17   13   27   41    4    7]\n",
      " [  15    5   96  135   56 4986   16  107   27   41    9   14]\n",
      " [   9    0   13   64   10   14 1189    1   27    3   10   71]\n",
      " [   4    3   35   31   10  318    5  483    6    4    6    6]\n",
      " [   2    0    5   65   18   33   18    1 1089    5    5    5]\n",
      " [  14    0   13   55   27   40    4    7    6 1199    6    6]\n",
      " [   1    0    0   10    5    7   20    1   14    0  357   15]\n",
      " [   4    0   11   38    8    8   60    5   19    3   12 1569]]\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    #actonclimate       0.78      0.74      0.76       568\n",
      "       #bushfires       0.85      0.84      0.84       270\n",
      "   #climateaction       0.72      0.63      0.67      1913\n",
      "   #climatechange       0.82      0.79      0.80      4857\n",
      "   #climatecrisis       0.73      0.67      0.70      1495\n",
      "   #climatestrike       0.83      0.91      0.86      5507\n",
      "     #environment       0.82      0.84      0.83      1411\n",
      "#fridaysforfuture       0.69      0.53      0.60       911\n",
      "   #globalwarming       0.79      0.87      0.83      1246\n",
      "    #greennewdeal       0.86      0.87      0.86      1377\n",
      "   #savetheplanet       0.82      0.83      0.82       430\n",
      "  #sustainability       0.88      0.90      0.89      1737\n",
      "\n",
      "         accuracy                           0.81     21722\n",
      "        macro avg       0.80      0.79      0.79     21722\n",
      "     weighted avg       0.81      0.81      0.81     21722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8087653070619648"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.810\n"
     ]
    }
   ],
   "source": [
    "print('%.3f' % 0.8101924316361293)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy is -- 0.8101924316361293 -- without tokens and stopwords\n",
    "- Accuracy is -- 0.8087653070619648 -- with tokenization and stopwords\n",
    "> Note adding or remove tokens and stopwords does not effect  accuracy much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 87818)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strVal = [\"Cuyahoga county eliminates use of plastic bags\",\"Weather is still warm in winters and is not freezing\"]\n",
    "Value = tf_vectorizer.transform(strVal)\n",
    "Value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svm.predict(Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#environment' '#climatechange']\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this way to obtain the value from encoding\n",
    "# inv_s = label_encoder.inverse_transform(predictions.astype(int).ravel())\n",
    "# inv_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tasne\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.84\n",
      "Best parameters:  {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1]}\n",
    "grid = GridSearchCV(LinearSVC(), param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best cross-validation score: 0.84\n",
    "- Best parameters:  {'C': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['#environment', '#climatechange'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.predict(Value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
